{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "O5-ur_FzWw3c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "KeyboardInterrupt\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # Disable TF debugg log\n",
        "\n",
        "# Auxiliar imports\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "\n",
        "# Model auxiliar imports\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.image import load_img\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "import keras_tuner\n",
        "\n",
        "# Model imports\n",
        "from tensorflow.keras.models import load_model, Sequential  # Load saved model\n",
        "from tensorflow.keras.optimizers import Adam  # Optimizer\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import MaxPooling2D\n",
        "from tensorflow.keras.datasets import mnist\n",
        "# from tensorflow.keras.applications import Res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Wb96MjUnMU8H"
      },
      "outputs": [],
      "source": [
        "def getSetDir(set=\"train\"):\n",
        "    dirs = [f\"./../dataset/archive/dataset/{set}/\"]\n",
        "    return dirs\n",
        "\n",
        "\n",
        "# Load dataset training data\n",
        "def loadData(set=\"train\", verbose=False, size=30):\n",
        "    data = []\n",
        "    labels = []\n",
        "\n",
        "    for dir in getSetDir(set):\n",
        "        for subdir in os.listdir(dir):\n",
        "            for img in os.listdir(dir + subdir)[0:size-1]:\n",
        "                arrayLikeImage = load_img(dir + subdir + \"/\" + img, target_size=(300, 300))\n",
        "                arrayLikeImage = img_to_array(arrayLikeImage)\n",
        "                data.append(arrayLikeImage)\n",
        "                labels.append(subdir)\n",
        "            if verbose:\n",
        "                print(f\"Done with [{subdir}] images.\")\n",
        "\n",
        "    return data, labels\n",
        "\n",
        "\n",
        "# trainData, trainLabels = loadData(\"train\", size=100)\n",
        "# testData, testLabels = loadData(\"test\", size=10)\n",
        "(trainData, trainLabels), (testData, testLabels) = mnist.load_data()\n",
        "(trainData, trainLabels), (testData, testLabels) = (trainData/255, trainLabels), (testData/255, testLabels)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trainData = np.expand_dims(trainData, -1)\n",
        "testData = np.expand_dims(testData, -1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xS0hBbfubUDd"
      },
      "outputs": [],
      "source": [
        "# One-Hot Encode dataset\n",
        "lb = LabelBinarizer()\n",
        "trainLabels = lb.fit_transform(trainLabels)\n",
        "# trainLabels = to_categorical(trainLabels)  # Turn each label into a binary array\n",
        "\n",
        "testLabels = lb.fit_transform(testLabels)\n",
        "# testLabels = to_categorical(testLabels)  # Turn each label into a binary array\n",
        "\n",
        "trainData = np.array(trainData, dtype=\"float32\")\n",
        "trainLabels = np.array(trainLabels)\n",
        "testData = np.array(testData, dtype=\"float32\")\n",
        "testLabels = np.array(testLabels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JBGkbyB-WnrL"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 28, 28, 1)]       0         \n",
            "                                                                 \n",
            " conv2d (Conv2D)             (None, 26, 26, 32)        320       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 8, 8, 32)         0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " flattten (Flatten)          (None, 2048)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 10)                20490     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 20,810\n",
            "Trainable params: 20,810\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Create Model\n",
        "input = Input(shape=trainData.shape[1:])\n",
        "kernelWindow = (3, 3)\n",
        "outputModel = Conv2D(64, kernel_size=kernelWindow[0], activation=\"relu\", input_shape=trainData[0].shape) (input)\n",
        "outputModel = MaxPooling2D(pool_size=kernelWindow) (outputModel)\n",
        "outputModel = Conv2D(32, kernel_size=kernelWindow[0], activation=\"relu\") (outputModel)\n",
        "outputModel = MaxPooling2D(pool_size=kernelWindow) (outputModel)\n",
        "outputModel = Flatten(name=\"flattten\") (outputModel)\n",
        "outputModel = Dense(128, activation=\"relu\") (outputModel)\n",
        "outputModel = Dropout(0.5) (outputModel)\n",
        "outputModel = Dense(10, activation=\"softmax\") (outputModel)\n",
        "\n",
        "model = Model(inputs=input, outputs = outputModel)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vw09VNZ-XbNe"
      },
      "outputs": [],
      "source": [
        "# Compile the model\n",
        "STEP_LEN = 1e-6\n",
        "EPOCHS = 5\n",
        "BATCH_SIZE = 30\n",
        "# opt = Adam(learning_rate=STEP_LEN, decay=STEP_LEN / EPOCHS)\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SS401NDOd1Ni"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 110s 57ms/step - loss: 0.2558 - accuracy: 0.9283 - val_loss: 0.1060 - val_accuracy: 0.9680\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 104s 55ms/step - loss: 0.0986 - accuracy: 0.9710 - val_loss: 0.0779 - val_accuracy: 0.9754\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 125s 66ms/step - loss: 0.0752 - accuracy: 0.9776 - val_loss: 0.0632 - val_accuracy: 0.9803\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 133s 71ms/step - loss: 0.0641 - accuracy: 0.9807 - val_loss: 0.0629 - val_accuracy: 0.9794\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 102s 54ms/step - loss: 0.0557 - accuracy: 0.9834 - val_loss: 0.0525 - val_accuracy: 0.9822\n"
          ]
        }
      ],
      "source": [
        "# Train\n",
        "# T = model.fit(\n",
        "#     x=trainData,\n",
        "#     y=trainLabels,\n",
        "#     validation_data=(testData, testLabels),\n",
        "#     epochs=EPOCHS)\n",
        "T = model.fit(\n",
        "    x=trainData,\n",
        "    y=trainLabels,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    steps_per_epoch=len(trainData) // BATCH_SIZE,\n",
        "    validation_data=(testData, testLabels),\n",
        "    validation_steps=len(testData) // BATCH_SIZE,\n",
        "    epochs=EPOCHS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "list(map(str, lb.classes_))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1U31_HSrYnmD",
        "outputId": "a700458e-3bc4-439e-ae40-ea2f2660c7f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "- [INFO] evaluating network...\n",
            "313/313 [==============================] - 10s 31ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.99      0.99       980\n",
            "           1       0.99      0.99      0.99      1135\n",
            "           2       0.98      0.97      0.98      1032\n",
            "           3       0.99      0.98      0.99      1010\n",
            "           4       0.98      0.99      0.98       982\n",
            "           5       0.99      0.99      0.99       892\n",
            "           6       0.99      0.99      0.99       958\n",
            "           7       0.97      0.98      0.97      1028\n",
            "           8       0.97      0.98      0.98       974\n",
            "           9       0.98      0.96      0.97      1009\n",
            "\n",
            "    accuracy                           0.98     10000\n",
            "   macro avg       0.98      0.98      0.98     10000\n",
            "weighted avg       0.98      0.98      0.98     10000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"- [INFO] evaluating network...\")\n",
        "predictedIdxs = model.predict(testData)\n",
        "predictedIdxs = np.argmax(predictedIdxs, axis=1)\n",
        "print(classification_report(testLabels.argmax(axis = 1), predictedIdxs, target_names = list(map(str, lb.classes_))))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.6 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
